# Постановка задачи

## Диалоговые данные

Диалогом будем называть следующий список: 
$$
d=[(u_1, s_1), \ldots, (u_n, s_n)],
$$
где $u_i$ — реплика участника диалога под номером $s_i$. Нас интересуют так называемые task-specific диалоги с двумя участниками: система и пользователь. С некоторой погрешностью их можно описать как диалоги между клиентом и работником сферы услуг (или роботом). У клиента по ходу диалога возникают различные намерения, которые работник старается выполнить. Это могут быть намерения найти ресторан и заказать столик в нем, вызывать такси или купить билет на поезд. Набор таких намерений в каком-то смысле является ДНК диалога., поскольку мы будем считать два диалога похожими, если они имеют схожий набор намерений.

## Аугментация

Под аугментацией будем понимать генерацию новых валидных примеров путем трансформации имеющихся. Валидными примерами называют те, которые в достаточной степени соответствуют реально встречающимся данным. Обозначим за $D$ множество валидных объектов. Тогда аугментацией является нетождественное отображение $\text{aug}(d)$, которое не выводит за пределы множества валидных объектов:
$$
\text{aug}: D\to D.
$$
Обычно такая генерация реализуется с помощью внесения небольших изменений в валидный тренировочный объект. Например, аугментациями изображения являются небольшой поворот или размытие. Аугментация текстов особенно сложна, поскольку валидность подразумевает соблюдение правил языка, осмысленность текста, соответствие некоторой стилистике. В случае диалогов необходимо поддерживать структуру и разделение ролей, как было обозначено в предыдущем подразделе

## Векторизация

Векторизация есть отображение $D$ в векторное пространство:
$$
f:D\to E\subseteq\R^d.
$$
Для объекта $d$ его векторизация (эмбеддинг) $f(d)=e$ должна передавать некоторое содержание $d$. Это отражается в том, что $e$ может содержать лексическую информацию (tf-idf, bag of words) или признаки, полезные для классификации и других downstream tasks. Особенно ценно, если с помощью эмбеддингов $f(a), f(b)$ можно оценивать семантическую близость объектов $a,b$.

Текстовый эмбеддер эффективнее всего обучать в виде параметрической модели $f_\theta$. Чтобы она выдавала богатое векторное представление, необходимо обучить ее выполнять те задачи, где эти признаки задействуются. Популярной задачей является контрастивное обучение:
$$
\mathcal{L}=-\log{\exp(\text{sim}(x,y))\over\sum_{z}\exp(\text{sim}(x,z))}.
$$
Здесь $x=f_\theta(a),y=f_\theta(b)$ — пара семантически похожих объектов, $z=f_\theta(c)$ — семантически далекие от $x$ объекты, а $\text{sim}$ — функция близости. Такая задача обучает модель $f_\theta$ отображать семантически близкие объекты в метрически близкие векторы. Наличие примеров семантически близких и далеких объектов дает возможность обучить неплохой эмбеддер с помощью контрастивного обучения.