@inproceedings{kour2014real,
  title={Real-time segmentation of on-line handwritten arabic script},
  author={Kour, George and Saabne, Raid},
  booktitle={Frontiers in Handwriting Recognition (ICFHR), 2014 14th International Conference on},
  pages={417--422},
  year={2014},
  organization={IEEE}
}

@inproceedings{kour2014fast,
  title={Fast classification of handwritten on-line Arabic characters},
  author={Kour, George and Saabne, Raid},
  booktitle={Soft Computing and Pattern Recognition (SoCPaR), 2014 6th International Conference of},
  pages={312--318},
  year={2014},
  organization={IEEE}
}

@article{hadash2018estimate,
  title={Estimate and Replace: A Novel Approach to Integrating Deep Neural Networks with Existing Applications},
  author={Hadash, Guy and Kermany, Einat and Carmeli, Boaz and Lavi, Ofer and Kour, George and Jacovi, Alon},
  journal={arXiv preprint arXiv:1804.09028},
  year={2018}
}

@INPROCEEDINGS{bow,
  author={Qader, Wisam A. and Ameen, Musa M. and Ahmed, Bilal I.},
  booktitle={2019 International Engineering Conference (IEC)}, 
  title={An Overview of Bag of Words;Importance, Implementation, Applications, and Challenges}, 
  year={2019},
  volume={},
  number={},
  pages={200-204},
  doi={10.1109/IEC47844.2019.8950616}}

@article{SprckJones2021ASI,
  title={A statistical interpretation of term specificity and its application in retrieval},
  author={Karen Sp{\"a}rck Jones},
  journal={J. Documentation},
  year={2021},
  volume={60},
  pages={493-502},
  url={https://api.semanticscholar.org/CorpusID:2996187}
}

@misc{mikolov2013efficient,
      title={Efficient Estimation of Word Representations in Vector Space}, 
      author={Tomas Mikolov and Kai Chen and Greg Corrado and Jeffrey Dean},
      year={2013},
      eprint={1301.3781},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{pennington-etal-2014-glove,
    title = "{G}lo{V}e: Global Vectors for Word Representation",
    author = "Pennington, Jeffrey  and
      Socher, Richard  and
      Manning, Christopher",
    booktitle = "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})",
    month = oct,
    year = "2014",
    address = "Doha, Qatar",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D14-1162",
    doi = "10.3115/v1/D14-1162",
    pages = "1532--1543",
}

@misc{mccann2018learned,
      title={Learned in Translation: Contextualized Word Vectors}, 
      author={Bryan McCann and James Bradbury and Caiming Xiong and Richard Socher},
      year={2018},
      eprint={1708.00107},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{peters-etal-2018-deep,
    title = "Deep Contextualized Word Representations",
    author = "Peters, Matthew E.  and
      Neumann, Mark  and
      Iyyer, Mohit  and
      Gardner, Matt  and
      Clark, Christopher  and
      Lee, Kenton  and
      Zettlemoyer, Luke",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N18-1202",
    doi = "10.18653/v1/N18-1202",
    pages = "2227--2237",
    abstract = "We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.",
}

@misc{xiao2023cpack,
      title={C-Pack: Packaged Resources To Advance General Chinese Embedding}, 
      author={Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighof},
      year={2023},
      eprint={2309.07597},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{wang2022text,
      title={Text Embeddings by Weakly-Supervised Contrastive Pre-training}, 
      author={Liang Wang and Nan Yang and Xiaolong Huang and Binxing Jiao and Linjun Yang and Daxin Jiang and Rangan Majumder and Furu Wei},
      year={2022},
      eprint={2212.03533},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{li2023general,
      title={Towards General Text Embeddings with Multi-stage Contrastive Learning}, 
      author={Zehan Li and Xin Zhang and Yanzhao Zhang and Dingkun Long and Pengjun Xie and Meishan Zhang},
      year={2023},
      eprint={2308.03281},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{zhang-etal-2023-dialog,
    title = "Dialog-Post: Multi-Level Self-Supervised Objectives and Hierarchical Model for Dialogue Post-Training",
    author = "Zhang, Zhenyu  and
      Shen, Lei  and
      Zhao, Yuming  and
      Chen, Meng  and
      He, Xiaodong",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.564",
    doi = "10.18653/v1/2023.acl-long.564",
    pages = "10134--10148",
    abstract = "Dialogue representation and understanding aim to convert conversational inputs into embeddings and fulfill discriminative tasks. Compared with free-form text, dialogue has two important characteristics, hierarchical semantic structure and multi-facet attributes. Therefore, directly applying the pretrained language models (PLMs) might result in unsatisfactory performance. Recently, several work focused on the dialogue-adaptive post-training (DialPost) that further trains PLMs to fit dialogues. To model dialogues more comprehensively, we propose a DialPost method, Dialog-Post, with multi-level self-supervised objectives and a hierarchical model. These objectives leverage dialogue-specific attributes and use self-supervised signals to fully facilitate the representation and understanding of dialogues. The novel model is a hierarchical segment-wise self-attention network, which contains inner-segment and inter-segment self-attention sub-layers followed by an aggregation and updating module. To evaluate the effectiveness of our methods, we first apply two public datasets for the verification of representation ability. Then we conduct experiments on a newly-labelled dataset that is annotated with 4 dialogue understanding tasks. Experimental results show that our method outperforms existing SOTA models and achieves a 3.3{\%} improvement on average.",
}

@misc{li2022future,
      title={Back to the Future: Bidirectional Information Decoupling Network for Multi-turn Dialogue Modeling}, 
      author={Yiyang Li and Hai Zhao and Zhuosheng Zhang},
      year={2022},
      eprint={2204.08152},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{soudani2023data,
  title={Data Augmentation for Conversational AI},
  author={Soudani, Heydar and Kanoulas, Evangelos and Hasibi, Faegheh},
  journal={arXiv preprint arXiv:2309.04739},
  year={2023}
}

@misc{zheng2023augesc,
      title={AugESC: Dialogue Augmentation with Large Language Models for Emotional Support Conversation}, 
      author={Chujie Zheng and Sahand Sabour and Jiaxin Wen and Zheng Zhang and Minlie Huang},
      year={2023},
      eprint={2202.13047},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{schick2021generating,
      title={Generating Datasets with Pretrained Language Models}, 
      author={Timo Schick and Hinrich Sch√ºtze},
      year={2021},
      eprint={2104.07540},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{mohapatra2021simulated,
      title={Simulated Chats for Building Dialog Systems: Learning to Generate Conversations from Instructions}, 
      author={Biswesh Mohapatra and Gaurav Pandey and Danish Contractor and Sachindra Joshi},
      year={2021},
      eprint={2010.10216},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@inproceedings{wan-etal-2022-unified,
    title = "A Unified Dialogue User Simulator for Few-shot Data Augmentation",
    author = "Wan, Dazhen  and
      Zhang, Zheng  and
      Zhu, Qi  and
      Liao, Lizi  and
      Huang, Minlie",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2022",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-emnlp.277",
    doi = "10.18653/v1/2022.findings-emnlp.277",
    pages = "3788--3799",
    abstract = "Pre-trained language models have shown superior performance in task-oriented dialogues. However, existing datasets are on limited scales, which cannot support large-scale pre-training. Fortunately, various data augmentation methods have been developed to augment large-scale task-oriented dialogue corpora. However, they heavily rely on annotated data in the target domain, which require a tremendous amount of data collection and human labeling work. In this paper, we build a unified dialogue user simulation model by pre-training on several publicly available datasets. The model can then be tuned on a target domain with few-shot data. The experiments on a target dataset across multiple domains show that our proposed model brings remarkable performance increases through data augmentation.",
}

@misc{kim2021neuralwoz,
      title={NeuralWOZ: Learning to Collect Task-Oriented Dialogue via Model-Based Simulation}, 
      author={Sungdong Kim and Minsuk Chang and Sang-Woo Lee},
      year={2021},
      eprint={2105.14454},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{vaswani2023attention,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2023},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{devlin2019bert,
      title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}, 
      author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
      year={2019},
      eprint={1810.04805},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{sennrich-etal-2016-neural,
    title = "Neural Machine Translation of Rare Words with Subword Units",
    author = "Sennrich, Rico  and
      Haddow, Barry  and
      Birch, Alexandra",
    booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P16-1162",
    doi = "10.18653/v1/P16-1162",
    pages = "1715--1725",
}

@misc{wu2016googles,
      title={Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation}, 
      author={Yonghui Wu and Mike Schuster and Zhifeng Chen and Quoc V. Le and Mohammad Norouzi and Wolfgang Macherey and Maxim Krikun and Yuan Cao and Qin Gao and Klaus Macherey and Jeff Klingner and Apurva Shah and Melvin Johnson and Xiaobing Liu and ≈Åukasz Kaiser and Stephan Gouws and Yoshikiyo Kato and Taku Kudo and Hideto Kazawa and Keith Stevens and George Kurian and Nishant Patil and Wei Wang and Cliff Young and Jason Smith and Jason Riesa and Alex Rudnick and Oriol Vinyals and Greg Corrado and Macduff Hughes and Jeffrey Dean},
      year={2016},
      eprint={1609.08144},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@InProceedings{TiedemannThottingal,
  author = {J{\"o}rg Tiedemann and Santhosh Thottingal},
  title = {{OPUS-MT} ‚Äî {B}uilding open translation services for the {W}orld},
  booktitle = {Proceedings of the 22nd Annual Conferenec of the European Association for Machine Translation (EAMT)},
  year = {2020},
  address = {Lisbon, Portugal}
 }

@Inproceedings{Zhou2022,
 author = {Zhihan Zhou and Dejiao Zhang and Wei Xiao and Nicholas Dingwall and Xiaofei Ma and Andrew O. Arnold and Bing Xiang},
 title = {Learning dialogue representations from consecutive utterances},
 year = {2022},
 url = {https://www.amazon.science/publications/learning-dialogue-representations-from-consecutive-utterances},
 booktitle = {NAACL 2022},
}

@misc{henderson2020convert,
      title={ConveRT: Efficient and Accurate Conversational Representations from Transformers}, 
      author={Matthew Henderson and I√±igo Casanueva and Nikola Mrk≈°iƒá and Pei-Hao Su and Tsung-Hsien Wen and Ivan Vuliƒá},
      year={2020},
      eprint={1911.03688},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{wolf2020huggingfaces,
      title={HuggingFace's Transformers: State-of-the-art Natural Language Processing}, 
      author={Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and R√©mi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush},
      year={2020},
      eprint={1910.03771},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{liu2019roberta,
      title={RoBERTa: A Robustly Optimized BERT Pretraining Approach}, 
      author={Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},
      year={2019},
      eprint={1907.11692},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{thakur2021augmented,
      title={Augmented SBERT: Data Augmentation Method for Improving Bi-Encoders for Pairwise Sentence Scoring Tasks}, 
      author={Nandan Thakur and Nils Reimers and Johannes Daxenberger and Iryna Gurevych},
      year={2021},
      eprint={2010.08240},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{zhang2023dialogstudio,
      title={DialogStudio: Towards Richest and Most Diverse Unified Dataset Collection for Conversational AI}, 
      author={Jianguo Zhang and Kun Qian and Zhiwei Liu and Shelby Heinecke and Rui Meng and Ye Liu and Zhou Yu and and Huan Wang and Silvio Savarese and Caiming Xiong},
      year={2023},
      eprint={2307.10172},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{grill2020bootstrap,
      title={Bootstrap your own latent: A new approach to self-supervised Learning}, 
      author={Jean-Bastien Grill and Florian Strub and Florent Altch√© and Corentin Tallec and Pierre H. Richemond and Elena Buchatskaya and Carl Doersch and Bernardo Avila Pires and Zhaohan Daniel Guo and Mohammad Gheshlaghi Azar and Bilal Piot and Koray Kavukcuoglu and R√©mi Munos and Michal Valko},
      year={2020},
      eprint={2006.07733},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{xiao2022retromae,
      title={RetroMAE: Pre-Training Retrieval-oriented Language Models Via Masked Auto-Encoder}, 
      author={Shitao Xiao and Zheng Liu and Yingxia Shao and Zhao Cao},
      year={2022},
      eprint={2205.12035},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{muennighoff2023mteb,
      title={MTEB: Massive Text Embedding Benchmark}, 
      author={Niklas Muennighoff and Nouamane Tazi and Lo√Øc Magne and Nils Reimers},
      year={2023},
      eprint={2210.07316},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{lee2019latent,
      title={Latent Retrieval for Weakly Supervised Open Domain Question Answering}, 
      author={Kenton Lee and Ming-Wei Chang and Kristina Toutanova},
      year={2019},
      eprint={1906.00300},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{liu2022dial2vec,
      title={Dial2vec: Self-Guided Contrastive Learning of Unsupervised Dialogue Embeddings}, 
      author={Che Liu and Rui Wang and Junfeng Jiang and Yongbin Li and Fei Huang},
      year={2022},
      eprint={2210.15332},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{zang2020multiwoz,
      title={MultiWOZ 2.2 : A Dialogue Dataset with Additional Annotation Corrections and State Tracking Baselines}, 
      author={Xiaoxue Zang and Abhinav Rastogi and Srinivas Sunkara and Raghav Gupta and Jianguo Zhang and Jindong Chen},
      year={2020},
      eprint={2007.12720},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{wu2018unsupervised,
      title={Unsupervised Feature Learning via Non-Parametric Instance-level Discrimination}, 
      author={Zhirong Wu and Yuanjun Xiong and Stella Yu and Dahua Lin},
      year={2018},
      eprint={1805.01978},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{oord2019representation,
      title={Representation Learning with Contrastive Predictive Coding}, 
      author={Aaron van den Oord and Yazhe Li and Oriol Vinyals},
      year={2019},
      eprint={1807.03748},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{noroozi2017unsupervised,
      title={Unsupervised Learning of Visual Representations by Solving Jigsaw Puzzles}, 
      author={Mehdi Noroozi and Paolo Favaro},
      year={2017},
      eprint={1603.09246},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{doersch2017multitask,
      title={Multi-task Self-Supervised Visual Learning}, 
      author={Carl Doersch and Andrew Zisserman},
      year={2017},
      eprint={1708.07860},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@inproceedings{gao-callan-2021-condenser,
    title = "Condenser: a Pre-training Architecture for Dense Retrieval",
    author = "Gao, Luyu  and
      Callan, Jamie",
    editor = "Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.75",
    doi = "10.18653/v1/2021.emnlp-main.75",
    pages = "981--993",
    abstract = "Pre-trained Transformer language models (LM) have become go-to text representation encoders. Prior research fine-tunes deep LMs to encode text sequences such as sentences and passages into single dense vector representations for efficient text comparison and retrieval. However, dense encoders require a lot of data and sophisticated techniques to effectively train and suffer in low data situations. This paper finds a key reason is that standard LMs{'} internal attention structure is not ready-to-use for dense encoders, which needs to aggregate text information into the dense representation. We propose to pre-train towards dense encoder with a novel Transformer architecture, Condenser, where LM prediction CONditions on DENSE Representation. Our experiments show Condenser improves over standard LM by large margins on various text retrieval and similarity tasks.",
}

@misc{xiong2020approximate,
      title={Approximate Nearest Neighbor Negative Contrastive Learning for Dense Text Retrieval}, 
      author={Lee Xiong and Chenyan Xiong and Ye Li and Kwok-Fung Tang and Jialin Liu and Paul Bennett and Junaid Ahmed and Arnold Overwijk},
      year={2020},
      eprint={2007.00808},
      archivePrefix={arXiv},
      primaryClass={cs.IR}
}

@misc{neelakantan2022text,
      title={Text and Code Embeddings by Contrastive Pre-Training}, 
      author={Arvind Neelakantan and Tao Xu and Raul Puri and Alec Radford and Jesse Michael Han and Jerry Tworek and Qiming Yuan and Nikolas Tezak and Jong Wook Kim and Chris Hallacy and Johannes Heidecke and Pranav Shyam and Boris Power and Tyna Eloundou Nekoul and Girish Sastry and Gretchen Krueger and David Schnurr and Felipe Petroski Such and Kenny Hsu and Madeleine Thompson and Tabarak Khan and Toki Sherbakov and Joanne Jang and Peter Welinder and Lilian Weng},
      year={2022},
      eprint={2201.10005},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{feng2022languageagnostic,
      title={Language-agnostic BERT Sentence Embedding}, 
      author={Fangxiaoyu Feng and Yinfei Yang and Daniel Cer and Naveen Arivazhagan and Wei Wang},
      year={2022},
      eprint={2007.01852},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{reimers2019sentencebert,
      title={Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks}, 
      author={Nils Reimers and Iryna Gurevych},
      year={2019},
      eprint={1908.10084},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{fang2020cert,
      title={CERT: Contrastive Self-supervised Learning for Language Understanding}, 
      author={Hongchao Fang and Sicheng Wang and Meng Zhou and Jiayuan Ding and Pengtao Xie},
      year={2020},
      eprint={2005.12766},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{yan-etal-2021-consert,
    title = "{C}on{SERT}: A Contrastive Framework for Self-Supervised Sentence Representation Transfer",
    author = "Yan, Yuanmeng  and
      Li, Rumei  and
      Wang, Sirui  and
      Zhang, Fuzheng  and
      Wu, Wei  and
      Xu, Weiran",
    editor = "Zong, Chengqing  and
      Xia, Fei  and
      Li, Wenjie  and
      Navigli, Roberto",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.393",
    doi = "10.18653/v1/2021.acl-long.393",
    pages = "5065--5075",
    abstract = "Learning high-quality sentence representations benefits a wide range of natural language processing tasks. Though BERT-based pre-trained language models achieve high performance on many downstream tasks, the native derived sentence representations are proved to be collapsed and thus produce a poor performance on the semantic textual similarity (STS) tasks. In this paper, we present ConSERT, a Contrastive Framework for Self-Supervised SEntence Representation Transfer, that adopts contrastive learning to fine-tune BERT in an unsupervised and effective way. By making use of unlabeled texts, ConSERT solves the collapse issue of BERT-derived sentence representations and make them more applicable for downstream tasks. Experiments on STS datasets demonstrate that ConSERT achieves an 8{\%} relative improvement over the previous state-of-the-art, even comparable to the supervised SBERT-NLI. And when further incorporating NLI supervision, we achieve new state-of-the-art performance on STS tasks. Moreover, ConSERT obtains comparable results with only 1000 samples available, showing its robustness in data scarcity scenarios.",
}

@misc{luo2021unsupervised,
      title={Unsupervised Document Embedding via Contrastive Augmentation}, 
      author={Dongsheng Luo and Wei Cheng and Jingchao Ni and Wenchao Yu and Xuchao Zhang and Bo Zong and Yanchi Liu and Zhengzhang Chen and Dongjin Song and Haifeng Chen and Xiang Zhang},
      year={2021},
      eprint={2103.14542},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{liu2021dialoguecse,
      title={DialogueCSE: Dialogue-based Contrastive Learning of Sentence Embeddings}, 
      author={Che Liu and Rui Wang and Jinghua Liu and Jian Sun and Fei Huang and Luo Si},
      year={2021},
      eprint={2109.12599},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@InProceedings{dgac,
author="Nagovitsin, Mark
and Kuznetsov, Denis",
editor="Kryzhanovsky, Boris
and Dunin-Barkowski, Witali
and Redko, Vladimir
and Tiumentsev, Yury",
title="DGAC: Dialogue Graph Auto Construction Based on¬†Data with¬†a¬†Regular Structure",
booktitle="Advances in Neural Computation, Machine Learning, and Cognitive Research VI",
year="2023",
publisher="Springer International Publishing",
address="Cham",
pages="508--529",
abstract="A scripted graph is a way to represent a dialogue scenario in a dialogue system. It is often used in development when the dialogue has a regular structure. In this paper, we propose a method for recovering or extracting regular structures from the data by building a dialogue graph. The dialogue graphs constructed by our method can be used for a more accurate pre-selection of candidates for response selection models on the MultiWOZ dataset. Quality improvements are demonstrated for various response selection models: statistical, pre-trained, and fine-tuned. Obtained results demonstrate the applicability of our approach to the automatic construction of a dialogue graph for the tasks of creating scenario-driven dialogue assistants and improving response selection models.",
isbn="978-3-031-19032-2"
}
