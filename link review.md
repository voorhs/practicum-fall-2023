| bib                                                          | topic                                | idea                                                         | name                   | read? |
| ------------------------------------------------------------ | ------------------------------------ | ------------------------------------------------------------ | ---------------------- | ----- |
| Nils Reimers, Iryna Gurevych: “Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks”, 2019; [arXiv:1908.10084](http://arxiv.org/abs/1908.10084). | text embedding                       | train bert mean pooling on NLI data, applicable for STS, pair clf etc. | SBERT                  |       |
| Tianyu Gao, Xingcheng Yao, Danqi Chen: “SimCSE: Simple Contrastive Learning of Sentence Embeddings”, 2021; [arXiv:2104.08821](http://arxiv.org/abs/2104.08821). | text embedding (STS)                 | train bert cls token on two augmentations of the same text (simple in-batch contrastive loss) + experiment with NLI data | SimSCE                 |       |
| Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, Edouard Grave: “Unsupervised Dense Information Retrieval with Contrastive Learning”, 2021; [arXiv:2112.09118](http://arxiv.org/abs/2112.09118). | text embedding (retrieval)           | same but uses MoCo + extensive analysis of existing text embedding methods | Contriever             |       |
| Luyu Gao, Jamie Callan: “Unsupervised Corpus Aware Language Model Pre-training for Dense Passage Retrieval”, 2021; [arXiv:2108.05540](http://arxiv.org/abs/2108.05540). | text embedding (retrieval)           | contrastive loss over spans from passages (query-agnostic) + sophisticated mixture with MLM | coCondenser            |       |
| Shitao Xiao, Zheng Liu, Yingxia Shao, Zhao Cao: “RetroMAE: Pre-Training Retrieval-oriented Language Models Via Masked Auto-Encoder”, 2022; [arXiv:2205.12035](http://arxiv.org/abs/2205.12035). | LM pre-train                         | drop sentence tokens and decode them from cls token          | RetroMAE               |       |
| Luyu Gao, Jamie Callan: “Condenser: a Pre-training Architecture for Dense Retrieval”, 2021; [arXiv:2104.08253](http://arxiv.org/abs/2104.08253). | LM pre-train                         | skip connect sentence tokens to decoder and condition MLM on cls token | Condenser              |       |
| Shitao Xiao, Zheng Liu, Peitian Zhang, Niklas Muennighof: “C-Pack: Packaged Resources To Advance General Chinese Embedding”, 2023; [arXiv:2309.07597](http://arxiv.org/abs/2309.07597). | text embedding                       | retromae on plain text, contrastive loss with large batch on weakly supervised data, contrastive loss with hard negatives on supervised data | BGE                    |       |
| Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, Furu Wei: “Text Embeddings by Weakly-Supervised Contrastive Pre-training”, 2022; [arXiv:2212.03533](http://arxiv.org/abs/2212.03533). | text embedding                       | almost the same                                              | E5                     |       |
| Zehan Li, Xin Zhang, Yanzhao Zhang, Dingkun Long, Pengjun Xie, Meishan Zhang: “Towards General Text Embeddings with Multi-stage Contrastive Learning”, 2023; [arXiv:2308.03281](http://arxiv.org/abs/2308.03281). | text embedding                       | almost the same                                              | GTE                    |       |
| Zhihan Zhou, Dejiao Zhang, Wei Xiao, Nicholas Dingwall, Xiaofei Ma, Andrew O. Arnold, Bing Xiang: “Learning Dialogue Representations from Consecutive Utterances”, 2022; [arXiv:2205.13568](http://arxiv.org/abs/2205.13568). | utterance embedding                  | contrastive loss on consecutive utterances, simulation of hard negatives via proximity weights | Amazon-DSE             |       |
| [Dialog-Post: Multi-Level Self-Supervised Objectives and Hierarchical Model for Dialogue Post-Training](https://aclanthology.org/2023.acl-long.564) (Zhang et al., ACL 2023) | dialogue LM                          | hierarchical self-attention & multitask learning             | dialogue-post, HSSA    |       |
| Yiyang Li, Hai Zhao, Zhuosheng Zhang: “Back to the Future: Bidirectional Information Decoupling Network for Multi-turn Dialogue Modeling”, 2022; [arXiv:2204.08152](http://arxiv.org/abs/2204.08152). | dialogue LM                          | three streams of attention: future2current, current2current, past2current + mixture of experts | BiDeN                  |       |
| Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, YaGuang Li, Hongrae Lee, Huaixiu Steven Zheng, Amin Ghafouri, Marcelo Menegali, Yanping Huang, Maxim Krikun, Dmitry Lepikhin, James Qin, Dehao Chen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts, Maarten Bosma, Vincent Zhao, Yanqi Zhou, Chung-Ching Chang, Igor Krivokon, Will Rusch, Marc Pickett, Pranesh Srinivasan, Laichee Man, Kathleen Meier-Hellstern, Meredith Ringel Morris, Tulsee Doshi, Renelito Delos Santos, Toju Duke, Johnny Soraker, Ben Zevenbergen, Vinodkumar Prabhakaran, Mark Diaz, Ben Hutchinson, Kristen Olson, Alejandra Molina, Erin Hoffman-John, Josh Lee, Lora Aroyo, Ravi Rajakumar, Alena Butryna, Matthew Lamm, Viktoriya Kuzmina, Joe Fenton, Aaron Cohen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera-Arcas, Claire Cui, Marian Croak, Ed Chi, Quoc Le: “LaMDA: Language Models for Dialog Applications”, 2022; [arXiv:2201.08239](http://arxiv.org/abs/2201.08239). | dialogue CLM                         | CLM on dialogue data scrapped from web (1.36T tokens)        | LaMDA                  |       |
| Wenxiang Jiao, Michael R. Lyu, Irwin King: “PT-CoDE: Pre-trained Context-Dependent Encoder for Utterance-level Emotion Recognition”, 2019; [arXiv:1910.08916](http://arxiv.org/abs/1910.08916). | utterance encoder                    | RNN within utterance -> mean pool -> RNN over utterance encodings; pretrain: mask out random utterance and rank candidates for this place | CoDE                   |       |
| Zhenyu Zhang, Tao Guo, Meng Chen: “DialogueBERT: A Self-Supervised Learning based Dialogue Pre-training Encoder”, 2021; [arXiv:2109.10480](http://arxiv.org/abs/2109.10480). | dialogue LM and encoder              | multitask learning, CNN pooler                               | DialogueBERT           |       |
| [Dialogue-oriented Pre-training](https://aclanthology.org/2021.findings-acl.235) (Xu & Zhao, Findings 2021) | dialogue LM                          | treat plain text as dialogue data and do multitask learning  | Dialog-PrLM            |       |
| Jitin Krishnan, Antonios Anastasopoulos, Hemant Purohit, Huzefa Rangwala: “Multilingual Code-Switching for Zero-Shot Cross-Lingual Intent Prediction and Slot Filling”, 2021; [arXiv:2103.07792](http://arxiv.org/abs/2103.07792). | augmentation                         | switch language multiple times within one sentence to make your model multilingual | Code-switching         |       |
| Keya AJ, Wadud MAH, Mridha MF, Alatiyyah M, Hamid MA. AugFake-BERT: Handling Imbalance through Augmentation of Fake News Using BERT to Enhance the Performance of Fake News Classification. *Applied Sciences*. 2022; 12(17):8398. https://doi.org/10.3390/app12178398 | fake news detection                  | replace and insert using MLM (scarce data in bengali lang)   | AugFake-BERT           |       |
| Bucos M, Țucudean G. Text Data Augmentation Techniques for Fake News Detection in the Romanian Language. *Applied Sciences*. 2023; 13(13):7389. https://doi.org/10.3390/app13137389 | fake news detection                  | easy data augmentation and back translation                  |                        |       |
| Wilton O. Júnior, Mauricio S. da Cruz, Andre Brasil Vieira Wyzykowski, Arnaldo Bispo de Jesus: “The use of Data Augmentation as a technique for improving neural network accuracy in detecting fake news about COVID-19”, 2022; [arXiv:2205.00452](http://arxiv.org/abs/2205.00452). | fake news detection                  | something boring                                             |                        |       |
| Kang Min Yoo, Dongju Park, Jaewook Kang, Sang-Woo Lee, Woomyeong Park: “GPT3Mix: Leveraging Large-scale Language Models for Text Augmentation”, 2021; [arXiv:2104.08826](http://arxiv.org/abs/2104.08826). | augmentation                         | LLM for augmenting                                           | GPT3Mix                | +     |
| Varun Kumar, Ashutosh Choudhary, Eunah Cho: “Data Augmentation using Pre-trained Transformer Models”, 2020; [arXiv:2003.02245](http://arxiv.org/abs/2003.02245). | augmentation                         | LM for data augmentaion (BERT, GPT, BART)                    |                        | +     |
| [Sequence-Level Mixed Sample Data Augmentation](https://aclanthology.org/2020.emnlp-main.447) (Guo et al., EMNLP 2020) | augmentation                         | mixup                                                        |                        | +     |
| Zach Wood-Doughty, Ilya Shpitser, Mark Dredze: “Generating Synthetic Text Data to Evaluate Causal Inference Methods”, 2021; [arXiv:2102.05638](http://arxiv.org/abs/2102.05638). | dataset generation                   | generate whole dataset (суть метода не понял)                |                        | +     |
| Chujie Zheng, Sahand Sabour, Jiaxin Wen, Zheng Zhang, Minlie Huang: “AugESC: Dialogue Augmentation with Large Language Models for Emotional Support Conversation”, 2022; [arXiv:2202.13047](http://arxiv.org/abs/2202.13047). | dia dataset generation, augmentation | dialogue datasert from reconstructing dialogues with LLM     | AugESC                 | +     |
| Timo Schick, Hinrich Schütze: “Generating Datasets with Pretrained Language Models”, 2021; [arXiv:2104.07540](http://arxiv.org/abs/2104.07540). | dataset generation                   | LLM -> STS dataset                                           |                        | +     |
| Biswesh Mohapatra, Gaurav Pandey, Danish Contractor, Sachindra Joshi: “Simulated Chats for Building Dialog Systems: Learning to Generate Conversations from Instructions”, 2020; [arXiv:2010.10216](http://arxiv.org/abs/2010.10216). | dia dataset generation               | make two LLM chat with each other                            |                        |       |
| [A Unified Dialogue User Simulator for Few-shot Data Augmentation](https://aclanthology.org/2022.findings-emnlp.277) (Wan et al., Findings 2022) | dia dataset generation               | the same but a little more info is provided to each LLM      |                        |       |
| Sungdong Kim, Minsuk Chang, Sang-Woo Lee: “NeuralWOZ: Learning to Collect Task-Oriented Dialogue via Model-Based Simulation”, 2021; [arXiv:2105.14454](http://arxiv.org/abs/2105.14454). | diataset generation                  | the same                                                     |                        |       |
| Heydar Soudani, Evangelos Kanoulas, Faegheh Hasibi: “Data Augmentation for Conversational AI”, 2023; [arXiv:2309.04739](http://arxiv.org/abs/2309.04739). DOI: [10.1145/3583780.3615291](https://dx.doi.org/10.1145/3583780.3615291). | survey, augmentation                 | list of papers with novel data augmentations                 |                        |       |
| John X. Morris, Eli Lifland, Jin Yong Yoo, Jake Grigsby, Di Jin, Yanjun Qi: “TextAttack: A Framework for Adversarial Attacks, Data Augmentation, and Adversarial Training in NLP”, 2020; [arXiv:2005.05909](http://arxiv.org/abs/2005.05909). | package, augmentation, text attach   | implementations of efficient augmentations                   | TextAttack             |       |
| Dianqi Li, Yizhe Zhang, Hao Peng, Liqun Chen, Chris Brockett, Ming-Ting Sun, Bill Dolan: “Contextualized Perturbation for Textual Adversarial Attack”, 2020; [arXiv:2009.07502](http://arxiv.org/abs/2009.07502). | augmentation                         | sophisticated search for insertions, replacements etc        | CLARE                  |       |
| Marco Tulio Ribeiro, Tongshuang Wu, Carlos Guestrin, Sameer Singh: “Beyond Accuracy: Behavioral Testing of NLP models with CheckList”, 2020, Association for Computational Linguistics (ACL), 2020; [arXiv:2005.04118](http://arxiv.org/abs/2005.04118). | text attack                          | change facts and NERs                                        | CheckList              |       |
| Jason Wei, Kai Zou: “EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks”, 2019; [arXiv:1901.11196](http://arxiv.org/abs/1901.11196). | augmentation                         | simple context-unaware augmentation                          | Easy Data Augmentation |       |
| Siddhant Garg, Goutham Ramakrishnan: “BAE: BERT-based Adversarial Examples for Text Classification”, 2020; [arXiv:2004.01970](http://arxiv.org/abs/2004.01970). DOI: [10.18653/v1/2020.emnlp-main.498](https://dx.doi.org/10.18653/v1/2020.emnlp-main.498). | text attack                          | MLM for replacements                                         | BAE                    |       |
|                                                              |                                      |                                                              |                        |       |
|                                                              |                                      |                                                              |                        |       |
|                                                              |                                      |                                                              |                        |       |
|                                                              |                                      |                                                              |                        |       |
|                                                              |                                      |                                                              |                        |       |
|                                                              |                                      |                                                              |                        |       |

